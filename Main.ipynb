{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "@author: sumanyumuku98\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "from numpy import moveaxis\n",
    "from skimage import io, transform\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.nn import BatchNorm1d\n",
    "\n",
    "import plotly\n",
    "# import bokeh\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# COnverting the Multi-Class Problem into Binary-CLass\n",
    "\n",
    "# C0: Undistracted driving\n",
    "# C1-C9: Distracted driving\n",
    "# \"\"\"\n",
    "# INFO_FILE = pd.read_csv(\"state-farm-distracted-driver-detection/driver_imgs_list.csv\")\n",
    "# INFO_FILE['Binary_labels'] = INFO_FILE.apply(lambda row: 0 if row.classname=='c0' else 1, axis=1 )\n",
    "# NUM_CLASSES=2\n",
    "# INFO_FILE.head()\n",
    "# temp_dir='data/v1_cam1_no_split/'\n",
    "NUM_CLASSES=10\n",
    "CLASS_LABELS=['c' + str(i) for i in range(10)]\n",
    "CLASSES_LIST=['Drive Safe', 'Text Right', 'Talk Right', 'Text Left', 'Talk Left',\n",
    "              'Adjust Radio', 'Drink', 'Reach Behind', 'Hair & Makeup', 'Talk Passenger']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PREDIR = 'data/v2_cam1_cam2_ split_by_driver/'\n",
    "CAMERA1='Camera 1'\n",
    "CAMERA2='Camera 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Models\n",
    "#######################################################\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "for param in mobilenet.parameters():\n",
    "    param.requires_grad=False\n",
    "mobilenet.classifier[1] = torch.nn.Linear(1280,4096)\n",
    "mobilenet.classifier.add_module('2',torch.nn.ReLU(inplace=True))\n",
    "mobilenet.classifier.add_module('3',torch.nn.Linear(4096,4096))\n",
    "mobilenet.classifier.add_module('4',torch.nn.ReLU(inplace=True))\n",
    "mobilenet.classifier.add_module('5',torch.nn.Linear(4096,NUM_CLASSES))\n",
    "\n",
    "# mobilenet.classifier.add_module(\"2\",torch.nn.Softmax(dim=-1))\n",
    "########################################################\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False\n",
    "resnet.fc = torch.nn.Linear(2048,4096)\n",
    "resnetMod=torch.nn.Sequential(resnet,\n",
    "                             torch.nn.ReLU(inplace=True),\n",
    "                             torch.nn.Dropout(0.2,inplace=True),\n",
    "                             torch.nn.Linear(4096,4096),\n",
    "                             torch.nn.ReLU(inplace=True),\n",
    "                             torch.nn.Linear(4096,NUM_CLASSES))\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad=False\n",
    "vgg.classifier[6] = torch.nn.Linear(4096,4096)\n",
    "vgg.classifier.add_module('7',BatchNorm1d(4096))\n",
    "vgg.classifier.add_module('8',torch.nn.Linear(4096,2048))\n",
    "vgg.classifier.add_module('9',torch.nn.ReLU(inplace=True))\n",
    "# vgg.classifier.add_module('10',torch.nn.Dropout(0.3,inplace=True))\n",
    "vgg.classifier.add_module('10',torch.nn.Linear(2048,2048))\n",
    "vgg.classifier.add_module('11',torch.nn.ReLU(inplace=True))\n",
    "vgg.classifier.add_module('12',torch.nn.Linear(2048,512))\n",
    "vgg.classifier.add_module('13',torch.nn.ReLU(inplace=True))\n",
    "vgg.classifier.add_module('14',torch.nn.Linear(512,NUM_CLASSES))\n",
    "\n",
    "# vgg.classifier.add_module('11',torch.nn.)\n",
    "\n",
    "\n",
    "# vgg.classifier.add_module(\"softmax\",torch.nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(resnetMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = models.squeezenet1_1(pretrained=True)\n",
    "# print(squeezenet)\n",
    "for param in squeezenet.parameters():\n",
    "    param.requires_grad=False\n",
    "squeezenet.classifier[1]=torch.nn.Conv2d(512,1024,kernel_size=(1,1),stride=(1,1))\n",
    "squeezenetMod=torch.nn.Sequential(squeezenet,\n",
    "                                 torch.nn.Linear(1024,4096),\n",
    "                                 torch.nn.ReLU(inplace=True),\n",
    "                                 torch.nn.Linear(4096,4096),\n",
    "                                 torch.nn.ReLU(inplace=True),\n",
    "                                 torch.nn.Linear(4096,NUM_CLASSES))\n",
    "\n",
    "    \n",
    "#############################\n",
    "mnasnet = models.mnasnet1_0(pretrained=True)\n",
    "for param in mnasnet.parameters():\n",
    "    param.requires_grad=False\n",
    "mnasnet.classifier[1] = torch.nn.Linear(1280,4096)\n",
    "mnasnet.classifier.add_module(\"2\",torch.nn.ReLU(inplace=True))\n",
    "mnasnet.classifier.add_module(\"3\",torch.nn.Linear(4096,4096))\n",
    "# mnasnet.classifier.add_module(\"4\",torch.nn.Dropout(0.2,inplace=True))\n",
    "mnasnet.classifier.add_module(\"4\",torch.nn.ReLU(inplace=True))\n",
    "mnasnet.classifier.add_module(\"5\",torch.nn.Dropout(0.2,inplace=True))\n",
    "mnasnet.classifier.add_module(\"6\",torch.nn.Linear(4096,NUM_CLASSES))\n",
    "# mnasnet.classifier.add_module(\"2\",torch.nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in squeezenet.named_parameters():\n",
    "#     if param[1].requires_grad:\n",
    "#         print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess for pytorch pretrained models\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(sample):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    transform=  transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,])\n",
    "    \n",
    "    img = sample['image']\n",
    "    label = sample['label']\n",
    "    \n",
    "#     print(label, type(label))\n",
    "    \n",
    "    sample['image'] = transform(img)\n",
    "    sample['label'] = torch.as_tensor(label)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def preprocessVal(sample):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    transform=  transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,])\n",
    "    \n",
    "    img = sample['image']\n",
    "    label = sample['label']\n",
    "    \n",
    "#     print(label, type(label))\n",
    "    \n",
    "    sample['image'] = transform(img)\n",
    "    sample['label'] = torch.as_tensor(label)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "    \n",
    "class Driver_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,IMAGE_PREDIR,preprocess = None, mode='train',split=0.8):\n",
    "        \n",
    "        self.root_dir = IMAGE_PREDIR\n",
    "        self.transform = preprocess\n",
    "        self.mode=mode\n",
    "        self.split=split\n",
    "        self.totalTrainImages=[]\n",
    "        self.totalTrainLabels=[]\n",
    "        self.totalTestImages=[]\n",
    "        self.totalTestLabels=[]\n",
    "        self.images=None\n",
    "        self.labels=None\n",
    "        \n",
    "        for c in CLASS_LABELS:\n",
    "            for name in glob.glob(self.root_dir+CAMERA1+'/train/'+c+'/*.jpg'):\n",
    "                self.totalTrainImages.append(name)\n",
    "                self.totalTrainLabels.append(CLASS_LABELS.index(c))\n",
    "            for name in glob.glob(self.root_dir+CAMERA2+'/train/'+c+'/*.jpg'):\n",
    "                self.totalTrainImages.append(name)\n",
    "                self.totalTrainLabels.append(CLASS_LABELS.index(c))\n",
    "                \n",
    "            for name in glob.glob(self.root_dir+CAMERA1+'/test/'+c+'/*.jpg'):\n",
    "                self.totalTestImages.append(name)\n",
    "                self.totalTestLabels.append(CLASS_LABELS.index(c))\n",
    "            for name in glob.glob(self.root_dir+CAMERA2+'/test/'+c+'/*.jpg'):\n",
    "                self.totalTestImages.append(name)\n",
    "                self.totalTestLabels.append(CLASS_LABELS.index(c))\n",
    "        \n",
    "        shuffle_list=list(zip(self.totalTrainImages,self.totalTrainLabels))\n",
    "        random.shuffle(shuffle_list)\n",
    "        self.totalTrainImages,self.totalTrainLabels=zip(*shuffle_list)\n",
    "        \n",
    "        if self.mode=='train':\n",
    "            length=len(self.totalTrainImages)\n",
    "            self.images= self.totalTrainImages[:int(self.split*length)]\n",
    "            self.labels=self.totalTrainLabels[:int(self.split*length)]\n",
    "        \n",
    "        elif self.mode=='val':\n",
    "            length=len(self.totalTrainImages)\n",
    "            self.images= self.totalTrainImages[int(self.split*length):]\n",
    "            self.labels=self.totalTrainLabels[int(self.split*length):]\n",
    "        else:\n",
    "            self.images=self.totalTestImages\n",
    "            self.labels=self.totalTestLabels\n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "#         print(type(label))\n",
    "        img_name = self.images[idx]\n",
    "        img = Image.open(img_name)\n",
    "\n",
    "        sample = {'image': img, 'label': label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def listcheck(self):\n",
    "        return self.images,self.labels\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData= Driver_Dataset(IMAGE_PREDIR,preprocess,'train',0.95)\n",
    "valData=Driver_Dataset(IMAGE_PREDIR,preprocessVal,'val',0.95)\n",
    "trainLoader=DataLoader(trainData,batch_size=16,num_workers=2)\n",
    "valLoader=DataLoader(valData,batch_size=16,num_workers=2)\n",
    "# DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crossloss=torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,dataloaderList,epochs, device = 'cpu',saveName=None):\n",
    "    model.to(device)\n",
    "    trainloader=dataloaderList[0]\n",
    "    valloader=dataloaderList[1]\n",
    "    paramsToUpdate=[]\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            paramsToUpdate.append(param)\n",
    "    \n",
    "    optimizer=torch.optim.SGD(paramsToUpdate,lr=1e-3)\n",
    "\n",
    "    valAccList=[]\n",
    "    \n",
    "    print('Start Training...')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss=0\n",
    "        \n",
    "        for i_batch, data in enumerate(trainloader):\n",
    "            \n",
    "\n",
    "            images=Variable(data['image'],requires_grad=True).to(device)\n",
    "            labels=Variable(data['label']).to(device)\n",
    "            \n",
    "            outputs=model(images)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss=Crossloss(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss+=loss.item()\n",
    "            \n",
    "            # Print Training Statistics\n",
    "            del images,labels,outputs\n",
    "            \n",
    "            if i_batch%100==99:\n",
    "                print('[%2d,%5d] Training Loss: %.3f'  % \n",
    "                      (epoch+1,i_batch+1,running_loss/100))\n",
    "                running_loss=0\n",
    "                \n",
    "        print('Validating the model')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valPreds=[]\n",
    "            valLabels=[]\n",
    "            for i_batch, data in enumerate(valloader):\n",
    "                \n",
    "                images= data['image'].to(device)\n",
    "                labels= data['label'].numpy()\n",
    "                \n",
    "                preds=model(images)\n",
    "                preds=preds.cpu().numpy()\n",
    "                \n",
    "                pred_labels=np.argmax(preds,axis=-1)\n",
    "                valPreds.append(pred_labels)\n",
    "                valLabels.append(labels)\n",
    "            \n",
    "            valPreds=np.concatenate(valPreds)\n",
    "            valLabels=np.concatenate(valLabels)\n",
    "            size=valPreds.shape[0]\n",
    "            running_corrects=np.sum(valPreds==valLabels)\n",
    "        \n",
    "            acc=float(running_corrects)/size\n",
    "            valAccList.append(acc)\n",
    "            print('Accuracy after epoch %2d is %.3f' % (epoch+1,acc))\n",
    "#             print(valPreds)\n",
    "#             print(valLabels)\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(),'Model_weights/'+saveName+'.pt')\n",
    "    \n",
    "    return model,valAccList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "[ 1,  100] Training Loss: 2.292\n",
      "[ 1,  200] Training Loss: 2.285\n",
      "[ 1,  300] Training Loss: 2.275\n",
      "[ 1,  400] Training Loss: 2.272\n",
      "[ 1,  500] Training Loss: 2.259\n",
      "[ 1,  600] Training Loss: 2.256\n",
      "[ 1,  700] Training Loss: 2.256\n",
      "Validating the model\n",
      "Accuracy after epoch  1 is 0.236\n",
      "[ 2,  100] Training Loss: 2.237\n",
      "[ 2,  200] Training Loss: 2.242\n",
      "[ 2,  300] Training Loss: 2.234\n",
      "[ 2,  400] Training Loss: 2.237\n",
      "[ 2,  500] Training Loss: 2.226\n",
      "[ 2,  600] Training Loss: 2.229\n",
      "[ 2,  700] Training Loss: 2.239\n",
      "Validating the model\n",
      "Accuracy after epoch  2 is 0.236\n",
      "Finished Training\n",
      "Saving Model\n"
     ]
    }
   ],
   "source": [
    "trainedModel,valAcc=  train(vgg,[trainLoader,valLoader],2,device,'vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
